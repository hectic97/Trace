{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet+SENet CIFAR-10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvCuD6DBCyZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SENet(nn.Module):\n",
        "    def __init__(self,channel,reduction):\n",
        "        super(SENet,self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.dense_1 = nn.Linear(channel,channel//reduction,False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense_2 = nn.Linear(channel//reduction,channel,False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch = x.size(0) \n",
        "        ch = x.size(1)\n",
        "        out = self.avg_pool(x)\n",
        "        out = out.view(batch,ch)\n",
        "        out = self.dense_1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dense_2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.view(batch,ch,1,1)\n",
        "        return x * out.expand(x.size())\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "class LayerBlock(nn.Module):\n",
        "    def __init__(self,in_dim,out_dim,down=False):\n",
        "        super(LayerBlock,self).__init__()\n",
        "  \n",
        "        self.bn1 = nn.BatchNorm2d(out_dim)  \n",
        "        self.bn2 = nn.BatchNorm2d(out_dim)\n",
        "        self.bn3 = nn.BatchNorm2d(out_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_dim,out_dim,3,1,1,bias=False)\n",
        "        self.down = down\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim =out_dim\n",
        "        self.match_size = LambdaLayer(lambda x:F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, out_dim//4, out_dim//4), \"constant\", 0))\n",
        "        self.se_layer = nn.Sequential(SENet(self.out_dim,16)) \n",
        "        \n",
        "\n",
        "        if self.down:\n",
        "            self.conv1 = nn.Conv2d(in_dim,out_dim,3,2,1,bias=False)\n",
        "            self.layer = nn.Sequential(\n",
        "                self.conv1,\n",
        "                self.bn1,\n",
        "                self.relu,\n",
        "                self.conv2,\n",
        "                self.bn2\n",
        "\n",
        "            )\n",
        "            \n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_dim,out_dim,3,1,1,bias=False)\n",
        "            self.layer = nn.Sequential(\n",
        "                self.conv1,\n",
        "                self.bn1,\n",
        "                self.relu,\n",
        "                self.conv2,\n",
        "                self.bn2\n",
        "\n",
        "            )\n",
        "    \n",
        "    def forward(self,x):\n",
        "        if self.down:\n",
        "            down_x = self.match_size(x)\n",
        "            out = self.layer(x)\n",
        "            out = self.se_layer(out)\n",
        "            out = out + down_x\n",
        "        else:\n",
        "            out = self.layer(x)\n",
        "\n",
        "            if not x.size()==out.size():\n",
        "                x = self.match_size(x)\n",
        "            \n",
        "            out = self.se_layer(out)\n",
        "            out = out+x\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MyResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyResNet,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,16,3,1,1,bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            LayerBlock(16,16),\n",
        "            LayerBlock(16,16),\n",
        "            LayerBlock(16,16),\n",
        "            LayerBlock(16,16),\n",
        "            LayerBlock(16,16),\n",
        "            )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            LayerBlock(16,32,True),\n",
        "            LayerBlock(32,32),\n",
        "            LayerBlock(32,32),\n",
        "            LayerBlock(32,32),\n",
        "            LayerBlock(32,32),\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            LayerBlock(32,64,True),\n",
        "            LayerBlock(64,64),\n",
        "            LayerBlock(64,64),\n",
        "            LayerBlock(64,64),\n",
        "            LayerBlock(64,64),\n",
        "            \n",
        "        )\n",
        "        # self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.dense = nn.Linear(64,10)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        # x = self.avgpool(x)\n",
        "        x = F.avg_pool2d(x,x.size()[3])\n",
        "        x = x.view(x.size()[0],-1)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC6ixxfTDGJL",
        "colab_type": "code",
        "outputId": "f4908848-5812-4428-8ba0-28b53894e363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_size = 128\n",
        "transform_train = transforms.Compose([\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomCrop(32,padding=4),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=(0.485,0.456,0.406),std=(0.229,0.224,0.225)),\n",
        "\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "                            \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=(0.485,0.456,0.406),std=(0.229,0.224,0.225)),\n",
        "                                      \n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./',train=True,download=True,transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./',train=False,download=True,transform=transform_test)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "targets = train_set.targets\n",
        "train_idx ,valid_idx = train_test_split(np.arange(len(targets)),test_size=0.1,random_state=517,shuffle=True,stratify=targets)\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size = batch_size,sampler=train_sampler,pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(train_set,batch_size = batch_size,sampler=valid_sampler,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set,batch_size=batch_size)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = MyResNet().to(device)\n",
        "model = nn.DataParallel(model)\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1,momentum=0.9,weight_decay=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[100,150])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([4500, 4500, 4500, 4500, 4500, 4500, 4500, 4500, 4500, 4500]))\n",
            "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([500, 500, 500, 500, 500, 500, 500, 500, 500, 500]))\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrDqpOpODJpt",
        "colab_type": "code",
        "outputId": "81af01c9-4e6a-4957-aac8-854008b49893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "\n",
        "epochs = 200\n",
        "\n",
        "for i in range(epochs):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for j,[img,label] in enumerate(train_loader):\n",
        "        x = img.to(device)\n",
        "        y = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        _,output_index = torch.max(output,1)\n",
        "        loss = loss_func(output,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += y.size()[0]\n",
        "        correct += (output_index == y).sum().float()\n",
        "     \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    lr_scheduler.step() \n",
        "    if i % (epochs/10) == 0:\n",
        "        with torch.no_grad():\n",
        "                val_correct = 0\n",
        "                val_total = 0\n",
        "                for k,[val_img,val_label] in enumerate(val_loader):\n",
        "                        val_x = val_img.to(device)\n",
        "                        val_y = val_label.to(device)\n",
        "                        model.eval()\n",
        "                        val_output = model.forward(val_x)\n",
        "                        _,val_output_index = torch.max(val_output,1)\n",
        "                        val_loss = loss_func(val_output,val_y)\n",
        "                        val_total += val_y.size()[0]\n",
        "                        val_correct += (val_output_index==val_y).sum().float()\n",
        "        print('EPOCHS {:>3d}/{} | train_loss : {:.4f} | train_acc : {:.4f} | val_loss : {:.4f} | val_acc : {:.4f}'\\\n",
        "                    .format(i+1,epochs,loss,correct*100/total,val_loss,100*val_correct/val_total))  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCHS   1/200 | train_loss : 1.2735 | train_acc : 40.9467 | val_loss : 1.4731 | val_acc : 51.2600\n",
            "EPOCHS  21/200 | train_loss : 0.4923 | train_acc : 88.8200 | val_loss : 0.2069 | val_acc : 84.3800\n",
            "EPOCHS  41/200 | train_loss : 0.2980 | train_acc : 91.6511 | val_loss : 0.6688 | val_acc : 85.7600\n",
            "EPOCHS  61/200 | train_loss : 0.1099 | train_acc : 92.8444 | val_loss : 0.0152 | val_acc : 84.4200\n",
            "EPOCHS  81/200 | train_loss : 0.1256 | train_acc : 93.8000 | val_loss : 0.2999 | val_acc : 86.4800\n",
            "EPOCHS 101/200 | train_loss : 0.0619 | train_acc : 96.7978 | val_loss : 0.0615 | val_acc : 91.9400\n",
            "EPOCHS 121/200 | train_loss : 0.0344 | train_acc : 99.4711 | val_loss : 0.4205 | val_acc : 92.6400\n",
            "EPOCHS 141/200 | train_loss : 0.0129 | train_acc : 99.6844 | val_loss : 0.0920 | val_acc : 92.4400\n",
            "EPOCHS 161/200 | train_loss : 0.0044 | train_acc : 99.8400 | val_loss : 0.6109 | val_acc : 92.1000\n",
            "EPOCHS 181/200 | train_loss : 0.0087 | train_acc : 99.8800 | val_loss : 0.0332 | val_acc : 92.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEijshwuNGmI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79d77a89-24b5-45d4-ffeb-375fd04b4044"
      },
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    \n",
        "    for image,label in test_loader:\n",
        "        x = image.to(device)\n",
        "        y = label.to(device)\n",
        "\n",
        "        output = model.forward(x)\n",
        "        _,output_index = torch.max(output,1)\n",
        "        total += label.size(0)\n",
        "        correct += (output_index == y).sum().float()\n",
        "\n",
        "    print('Accuracy of Testset : {:.4f}'.format(100*correct/total))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Testset : 92.2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeYO9zpmqI4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}