{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples=[\"The cat sat on the mat.\", 'The dog ate my homework.']\n",
    "token_index={}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word]=len(token_index)+1\n",
    "            \n",
    "max_length=10\n",
    "results=np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n",
    "for i,sample in enumerate(samples): #sentense 1\n",
    "    for j,word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index=token_index.get(word)\n",
    "        results[i,j,index]=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "characters=string.printable # 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIFKLMNOPQRSTUVWXyZ...\n",
    "token_index=dict(zip(characters,range(1,len(characters)+1))) #{\"0\":1,'1':2 .... 'A':37 ...}\n",
    "max_length=50\n",
    "results=np.zeros((len(samples),max_length,max(token_index.values())+1))\n",
    "for i,sample in enumerate(samples):\n",
    "    for j,character in enumerate(sample):\n",
    "        index=token_index.get(character)\n",
    "        results[i,j,index]=1.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 101)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape #sample n , token n , all token n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer=Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9} 9\n"
     ]
    }
   ],
   "source": [
    "sequences=tokenizer.texts_to_sequences(samples)\n",
    "print(sequences)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary') \n",
    "word_index=tokenizer.word_index\n",
    "print(word_index,len(word_index))\n",
    "#tokenizer.texts_to_matrix == .texts_to_sequences + .sequences_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Hashing\n",
    "dimensionality=1000 # 단어를 크기가 1000인 벡터로 저장. 그 이상의 단어가 있다면 해싱 충돌이 발생 인코딩 정확도 감소\n",
    "max_length = 10\n",
    "\n",
    "results=np.zeros((len(samples),max_length,dimensionality))\n",
    "for i,sample in enumerate(samples):\n",
    "    for j,word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index=abs(hash(word))%dimensionality #0~1000사이의 랜덤한 정수 인덱스로 변환\n",
    "        results[i,j,index]=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer=Embedding(1000,64) #(num of token, dimension) word index ( 1~ 999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras  import preprocessing\n",
    "\n",
    "max_features=10000\n",
    "maxlen = 20\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words = max_features) #x_train 25000 comment each [15,78,49,82,79 ...]\n",
    "x_train2=preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen) #list(25000,) ->ndarray (25000,maxlen) \n",
    "x_test2=preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 12, 15, 16, 16, 16, 16, 19, 32, 32, 38, 65, 88, 103, 113, 178, 283, 1334, 4472, 5345] \n",
      "\n",
      "[5, 12, 15, 16, 16, 16, 16, 19, 32, 32, 38, 65, 88, 103, 113, 178, 283, 1334, 4472, 5345]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(x_train[0][-20:]),'\\n')\n",
    "print(sorted(x_train2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(10000,8,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "20000/20000 [==============================] - 2s 110us/step - loss: 0.6759 - acc: 0.6045 - val_loss: 0.6398 - val_acc: 0.6802\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 86us/step - loss: 0.5658 - acc: 0.7426 - val_loss: 0.5468 - val_acc: 0.7204\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 92us/step - loss: 0.4752 - acc: 0.7805 - val_loss: 0.5114 - val_acc: 0.7376\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 88us/step - loss: 0.4264 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7434\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.3932 - acc: 0.8256 - val_loss: 0.4981 - val_acc: 0.7542\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 88us/step - loss: 0.3672 - acc: 0.8392 - val_loss: 0.5013 - val_acc: 0.7532\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.3439 - acc: 0.8534 - val_loss: 0.5051 - val_acc: 0.7528\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 90us/step - loss: 0.3228 - acc: 0.8647 - val_loss: 0.5131 - val_acc: 0.7480\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 89us/step - loss: 0.3028 - acc: 0.8756 - val_loss: 0.5212 - val_acc: 0.7496\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.2845 - acc: 0.8858 - val_loss: 0.5300 - val_acc: 0.7488\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train2,y_train,epochs=10,batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir='../../data/aclImdb/aclImdb'\n",
    "train_dir=os.path.join(imdb_dir,'train')\n",
    "labels=[]\n",
    "texts=[]\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir,label_type)  #  dir /train_dir/label_type\n",
    "    for fname in os.listdir(dir_name):    # get  files & dir names in dir_name\n",
    "        if fname[-4:] == '.txt':\n",
    "            f=open(os.path.join(dir_name,fname),encoding='utf8')\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, list)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts) , type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582 unique tokens.\n",
      "Data Tensor Shape : (25000, 100)\n",
      "Labels Tensor Shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen=100\n",
    "training_samples=200\n",
    "validation_samples=10000\n",
    "max_words=10000 # 빈도높은 10000개 단어\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "print('%s unique tokens.' %len(word_index))\n",
    "\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "labels=np.asarray(labels) # copy=false shallow\n",
    "print('Data Tensor Shape :',data.shape)\n",
    "print('Labels Tensor Shape:',labels.shape)\n",
    "\n",
    "indices=np.arange(data.shape[0]) # num of documents\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels= labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples : training_samples+validation_samples]\n",
    "y_val=labels[training_samples : training_samples+validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 Word Vector.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '../../data/data/glove.6B'\n",
    "\n",
    "embedding_index={}\n",
    "f=open(os.path.join(glove_dir,'glove.6B.100d.txt'),encoding='utf8')\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word = values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "print('Found %s Word Vector.'%len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "for word, i in word_index.items(): # 'the' : 1, 'and' :2\n",
    "    if i<max_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.6330 - acc: 0.5250 - val_loss: 0.6959 - val_acc: 0.5127\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.7495 - acc: 0.5800 - val_loss: 0.6921 - val_acc: 0.5247\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5256 - acc: 0.7750 - val_loss: 1.0766 - val_acc: 0.4936\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5688 - acc: 0.6800 - val_loss: 0.8138 - val_acc: 0.5305\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4678 - acc: 0.7250 - val_loss: 0.7276 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1929 - acc: 0.9700 - val_loss: 1.4134 - val_acc: 0.4938\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1935 - acc: 0.9050 - val_loss: 0.9396 - val_acc: 0.5150\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0962 - acc: 0.9800 - val_loss: 2.4718 - val_acc: 0.4935\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1915 - acc: 0.9300 - val_loss: 0.7700 - val_acc: 0.5762\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.8375 - val_acc: 0.5733\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#OVERFITTING\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.6949 - acc: 0.4650 - val_loss: 0.6955 - val_acc: 0.5156\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4956 - acc: 0.9800 - val_loss: 0.7072 - val_acc: 0.5057\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2814 - acc: 0.9850 - val_loss: 0.7029 - val_acc: 0.5210\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1150 - acc: 1.0000 - val_loss: 0.7193 - val_acc: 0.5169\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0513 - acc: 1.0000 - val_loss: 0.7168 - val_acc: 0.5294\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.7286 - val_acc: 0.5256\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.7234 - val_acc: 0.5344\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.7426 - val_acc: 0.5262\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.7312 - val_acc: 0.5379\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7513 - val_acc: 0.5326\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=10,batch_size=32,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
