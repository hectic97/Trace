{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "chunk_len = 200\n",
    "hidden_size = 100\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "embedding_size = 70\n",
    "lr = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\r",
      "\u000b",
      "\f",
      "\n",
      "num_chars =  100\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "print(all_characters)\n",
    "print('num_chars = ', n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len =  1115394\n"
     ]
    }
   ],
   "source": [
    "file = open('shakespeare.txt',encoding='utf8').read()\n",
    "file_len = len(file)\n",
    "print('file_len = ',file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 37, 38, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "def random_chunk():\n",
    "    start_index = random.randint(0,file_len-chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "print(char_tensor(\"ABCdef\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers=1):\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size,embedding_size)\n",
    "        self.rnn = nn.RNN(embedding_size,hidden_size,num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self,input,hidden):\n",
    "        out = self.encoder(input.view(1,-1)) # row vector\n",
    "        out,hidden = self.rnn(out,hidden)\n",
    "        out = self.decoder(out.view(batch_size,-1))\n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=n_characters,embedding_size=embedding_size,hidden_size=hidden_size,\n",
    "           output_size = n_characters,num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = char_tensor(\"A\")\n",
    "hidden = model.init_hidden()\n",
    "out,hidden = model(inp,hidden)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    start_str = \"c\"\n",
    "    inp = char_tensor(start_str)\n",
    "    hidden = model.init_hidden()\n",
    "    x = inp\n",
    "\n",
    "    print(start_str,end=\"\")\n",
    "    for i in range(200):\n",
    "        output,hidden = model(x,hidden)\n",
    "\n",
    "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 만약 max 값만 쓰는 경우에\n",
    "        # 생성되는 텍스트가 다 the the the the the 이런식으로 나오기 때문입니다.\n",
    "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 램덤하게 다음 글자를 뽑아내는 방식으로 자연스러운 텍스트를 생성해냅니다.\n",
    "        output_dist = output.data.view(-1).div(0.8).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "\n",
    "        print(predicted_char,end=\"\")\n",
    "\n",
    "        x = char_tensor(predicted_char)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([2.3882], grad_fn=<DivBackward0>) \n",
      "\n",
      "c's to hir on at band or mang with nof ankef\n",
      "Moun bund.\n",
      "\n",
      "HIRI?NURTINNAr:\n",
      "And I an math er keryithiny,\n",
      "\n",
      "Krell bad difeat fellised deat ir orint minge !fant thane thar.\n",
      "\n",
      "WERNCEgERCCOOOY:\n",
      "An bite ! lorse \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2895], grad_fn=<DivBackward0>) \n",
      "\n",
      "cErid fsacle thou ghen haisgiel I sheressenr ihes the soy oule on thol and heswert hecond hit eet, wordans the in, cars foanbroy tho sall mefle with wiwh\n",
      "WoRrM, herat you had thed ther allit: the trowl\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1671], grad_fn=<DivBackward0>) \n",
      "\n",
      "cIED:\n",
      "A shary thele kererth's hime her in I jather hore then lum this the hacrritt,\n",
      "4o the bothoue gond hie seitf anes hiig fur bllave the soredin;\n",
      "And sares welte.\n",
      "\n",
      "FETS:\n",
      "Oe and inds oue theur nos Com\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1471], grad_fn=<DivBackward0>) \n",
      "\n",
      "chad, pould tones houbs in mong, wors bur?\n",
      "\n",
      "LAOMUDNGUS:\n",
      "Thy rowaver the me wathas maded, aning the hean?\n",
      "\n",
      "IUS:\n",
      "Hor wnokes.\n",
      "\n",
      "SIMELIA:\n",
      "Sok lond wichaf, in houd hi's grtengove sir the gint; thand Vporlr h\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2308], grad_fn=<DivBackward0>) \n",
      "\n",
      "cherh.\n",
      "\n",
      "LRRDANE Ghad vean lath aging will ur congest lall Cimened\n",
      "rive and serpind torand to nan: that of age my gomt rale mathist and, agumy you and bealm to sis my corgaters elling back and bettiln l\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0930], grad_fn=<DivBackward0>) \n",
      "\n",
      "cENI I ll, and seogseagh s idtion yoe be cove with proungno\n",
      "thus mith hith shou well and mave had the owakp in bres fur and you the the grang net hand ging fore my swish that bo sill that Tous, filind \n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.1695], grad_fn=<DivBackward0>) \n",
      "\n",
      "cI to me ford of dot serser corth perte dit bemy mevers you bendy to fore in will ot, your for fortert,\n",
      "Whet,\n",
      "No yourser, liin,\n",
      "Loust tare, her meard,\n",
      "What hord;\n",
      "Ka this pare my rovest the ards matij q\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.0656], grad_fn=<DivBackward0>) \n",
      "\n",
      "ches for hather:\n",
      "Tham sheve to nos my with the and heres lack word not tele blestore gow I pet.\n",
      "\n",
      "AUCENU]IRIO:\n",
      "Thou semem aod breath's nuve, the biegreans.\n",
      "\n",
      "Mave him, a weem sid wofching, a mad haest of\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([1.9880], grad_fn=<DivBackward0>) \n",
      "\n",
      "chistars inser whyel and words ent and This I the scacist my and you waom and sell, but not as ichast tsungy yow id fisees there'd snow sir dood, ward, the bering wiiss thag waed.\n",
      "\n",
      "ACICO:\n",
      "Heir chest th\n",
      " ====================================================================================================\n",
      "\n",
      " tensor([2.2522], grad_fn=<DivBackward0>) \n",
      "\n",
      "chifs redhele core come house our groked,\n",
      "Mow vall thin, will here to houle the amone and in the is tith what hod you lowe to death warst we toop himbut thou I wadd I thald githou, ploss ht ir that tha\n",
      " ====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    \n",
    "    total = char_tensor(random_chunk())\n",
    "    inp = total[:-1]\n",
    "    label = total[1:]\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
    "    optimizer.zero_grad()\n",
    "    for j in range(chunk_len-1):\n",
    "        x = inp[j]\n",
    "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
    "        y,hidden = model(x,hidden)\n",
    "        loss += loss_func(y,y_)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%50 == 0:\n",
    "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
    "        test()\n",
    "        print(\"\\n\",\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
