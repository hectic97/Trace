{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master\\\\Practice', 'C:\\\\Anaconda\\\\python37.zip', 'C:\\\\Anaconda\\\\DLLs', 'C:\\\\Anaconda\\\\lib', 'C:\\\\Anaconda', '', 'C:\\\\Anaconda\\\\lib\\\\site-packages', 'C:\\\\Anaconda\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Anaconda\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Anaconda\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Anaconda\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\집\\\\.ipython', '..', '../', '../..', '..', '../', '../../', '..', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master', 'C:\\\\Users\\\\집\\\\Notebook\\\\Deep_Learning\\\\deep-learning-from-scratch-2-master\\\\']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "print(sys.path)\n",
    "import numpy as np\n",
    "from common.functions import softmax\n",
    "from ch06.rnnlm import Rnnlm\n",
    "from ch06.better_rnnlm import BetterRnnlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1,1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())\n",
    "            \n",
    "            sampled = np.random.choice(len(p),size=1,p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "        \n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you chose introduced stocks whole backed christmas objective jon feel olivetti telephones heading immediate foot practically hanover scowcroft promptly lucrative reaching contributed streets prepayment eurocom establishing roller confessed failing mirror filipino bound kill victory downward unrest baseline ncaa vermont-slauson the increases bright jersey enhanced milton bottles players broker-dealer complains installed longtime backer maintained warrant secret seats infrastructure subject piano pence ahmanson mo. rode tisch heightened divided ounce bureaucracy defenses model strengths appreciate gregory ropes influenced burdens accelerating detectors pickers phone devaluation earn station dance vessel prescribed senate satisfaction nurses inviting fingers tracking referring baldwin jeans confirmed nurses vinson respectability refusal\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "corpus,word_to_id,id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id,skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you see bacteria are all or without a negative situation we will have charge here says madison vice president of exclusivity said.\n",
      " recently banco happens to buy some of these issues the value of the remaining current unfriendly shareholder for the touch of us waiting only century as far as the market appear to be thrown good said placed fully extremely weak management and expectations which is trying to provide societe ventures and regulatory relationships.\n",
      " last friday prices closed yesterday at least part of the index.\n",
      " the latest vision index traders often may not be set subsequent\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "corpus,word_to_id,id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('../ch06/Rnnlm.pkl')\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N','<unk>','$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "word_ids = model.generate(start_id,skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>','.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 0  7  2 11 11 12  5]\n",
      "[ 6  7  9 10  5]\n",
      "19+884 \n",
      "_903 \n"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "(x_train,t_train),(x_test,t_test) = sequence.load_data('addition.txt',seed=2020)\n",
    "char_to_id,id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape,t_train.shape)\n",
    "print(x_test.shape,t_test.shape)\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
    "print(''.join([id_to_char[c] for c in t_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
